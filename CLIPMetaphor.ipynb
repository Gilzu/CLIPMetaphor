{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerías y configuración del dispositivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPImageProcessor\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import gc\n",
    "#from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
    "# mostrar que dispositivo se esta usando\n",
    "torch.cuda.get_device_name(0)\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implicación de lenguaje visual (VLE)\n",
    "\n",
    "Contiene las funciones necesarias para la experimentación de VLE con las funciones de evaluación flexible y estrica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación VLE estricta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtainUniqueImages(images):\n",
    "    unique_images = []\n",
    "    for img in images:\n",
    "        if img not in unique_images:\n",
    "            unique_images.append(img)\n",
    "\n",
    "    return unique_images\n",
    "\n",
    "def format_VE_strict(dataframe, premises):\n",
    "    new_dataframe = dataframe.copy()\n",
    "\n",
    "    for premise in premises:\n",
    "        premise_data = new_dataframe[new_dataframe['Premise'] == premise]\n",
    "        # Obtener los los labels de la premisa\n",
    "        premise_labels = premise_data['Label'].unique()\n",
    "\n",
    "        if len(premise_labels) == 1:\n",
    "            # Si hay solo un tipo de etiqueta, eliminamos todas las filas con esa premisa\n",
    "            new_dataframe = new_dataframe[new_dataframe['Premise'] != premise]\n",
    "        elif len(premise_labels) == 3:\n",
    "            # Quedarme únicamente con los ejemplos de \"entailment\", \"neutral\" y \"contradiction\" una única vez\n",
    "            for label in [\"entailment\", \"neutral\", \"contradiction\"]:\n",
    "                label_data = premise_data[premise_data['Label'] == label]\n",
    "                indexes_data = label_data[\"Index\"].unique()\n",
    "                if len(indexes_data) > 1:\n",
    "                    for i in range(1, len(indexes_data)):\n",
    "                        new_dataframe.drop(new_dataframe[new_dataframe['Index'] == indexes_data[i]].index, inplace=True)\n",
    "        else:\n",
    "            # Si no cumple con ninguno de los criterios anteriores, eliminamos todas las filas con esa premisa\n",
    "            new_dataframe = new_dataframe[new_dataframe['Premise'] != premise]\n",
    "\n",
    "    return new_dataframe\n",
    "\n",
    "# Evaluación estrica\n",
    "def evaluateStrictVE(unique_imgs, df):\n",
    "    preprocess = CLIPImageProcessor()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for img in unique_imgs:\n",
    "            preprocessed_img = preprocess(img)\n",
    "            image = torch.tensor(np.array(preprocessed_img['pixel_values'])).to(device)\n",
    "            hypothesis = df['Hypothesis'][df['Image'] == img]\n",
    "            labels = df['Label'][df['Image'] == img]\n",
    "            image_features = model.encode_image(image).float()\n",
    "            tokenized_hypothesis = clip.tokenize(hypothesis).to(device)\n",
    "            text_features = model.encode_text(tokenized_hypothesis).float()\n",
    "            similarities = compute_similarities(text_features, image_features)\n",
    "\n",
    "            entailment_similarity = similarities[labels == 'entailment']\n",
    "            neutral_similarity = similarities[labels == 'neutral']\n",
    "            contradiction_similarity = similarities[labels == 'contradiction']\n",
    "\n",
    "            # Se tiene que cumplir que la similitud de entailment sea mayor que la de neutral y que la neutral sea mayor que la de contradicción\n",
    "            if entailment_similarity > neutral_similarity and neutral_similarity > contradiction_similarity:\n",
    "                correct += len(hypothesis) # Correcto para el número de hipótesis de la imagen (son 3 hipótesis, entailment, neutral y contradicción)\n",
    "            total += len(hypothesis)\n",
    "\n",
    "    print('total:', total)\n",
    "    print('Aciertos:', correct)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación VLE flexible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_VE_flexible(dataframe, premises):\n",
    "    new_dataframe = dataframe.copy()\n",
    "\n",
    "    for premise in premises:\n",
    "        premise_data = new_dataframe[new_dataframe['Premise'] == premise]\n",
    "        # Obtener los labels únicos de la premisa\n",
    "        premise_labels = premise_data['Label'].unique()\n",
    "\n",
    "        if len(premise_labels) == 1:\n",
    "            # Si hay solo un tipo de etiqueta, eliminamos todas las filas con esa premisa\n",
    "            new_dataframe = new_dataframe[new_dataframe['Premise'] != premise]\n",
    "        elif len(premise_labels) >= 2 and \"entailment\" in premise_labels:\n",
    "            # Conservar los ejemplos si hay al menos dos etiquetas y una de ellas es \"entailment\"\n",
    "            for label in premise_labels:\n",
    "                label_data = premise_data[premise_data['Label'] == label]\n",
    "                indexes_data = label_data[\"Index\"].unique()\n",
    "                if len(indexes_data) > 1:\n",
    "                    for i in range(1, len(indexes_data)):\n",
    "                        new_dataframe.drop(new_dataframe[new_dataframe['Index'] == indexes_data[i]].index, inplace=True)\n",
    "        else:\n",
    "            # Si no cumple con ninguno de los criterios anteriores, eliminamos todas las filas con esa premisa\n",
    "            new_dataframe = new_dataframe[new_dataframe['Premise'] != premise]\n",
    "\n",
    "    return new_dataframe\n",
    "\n",
    "\n",
    "def evaluateFlexibleVE(unique_imgs, df):\n",
    "    preprocess = CLIPImageProcessor()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for img in unique_imgs:\n",
    "            preprocessed_img = preprocess(img)\n",
    "            image = torch.tensor(np.array(preprocessed_img['pixel_values'])).to(device)\n",
    "            hypothesis = df['Hypothesis'][df['Image'] == img]\n",
    "            labels = df['Label'][df['Image'] == img]\n",
    "            image_features = model.encode_image(image).float()\n",
    "            tokenized_hypothesis = clip.tokenize(hypothesis).to(device)\n",
    "            text_features = model.encode_text(tokenized_hypothesis).float()\n",
    "            similarities = compute_similarities(text_features, image_features)\n",
    "\n",
    "            # La imagen puede tener 3 hipótesis cada una con su respectiva etiqueta (entailment, neutral, contradiction) o solo dos hipótesis (entailment y neutral o entailment y contradiction)\n",
    "            if len(labels) == 3:\n",
    "                entailment_similarity = similarities[labels == 'entailment']\n",
    "                neutral_similarity = similarities[labels == 'neutral']\n",
    "                contradiction_similarity = similarities[labels == 'contradiction']\n",
    "                \"\"\"\n",
    "                max_similarity = max(neutral_similarity, contradiction_similarity)\n",
    "\n",
    "                if entailment_similarity > max_similarity:\n",
    "                    correct += len(hypothesis)\n",
    "                \"\"\"\n",
    "                if entailment_similarity > neutral_similarity and entailment_similarity > contradiction_similarity:\n",
    "                    correct += len(hypothesis) # Correcto para el número de hipótesis de la imagen (son 3 hipótesis, entailment, neutral y contradicción)\n",
    "\n",
    "            elif len(labels) == 2:\n",
    "                entailment_similarity = similarities[labels == 'entailment']\n",
    "                other_similarity = similarities[(labels == 'neutral') | (labels == 'contradiction')]\n",
    "\n",
    "                if entailment_similarity > other_similarity:\n",
    "                    correct += len(hypothesis)\n",
    "\n",
    "            total += len(hypothesis)\n",
    "\n",
    "    print('total:', total)\n",
    "    print('Aciertos:', correct)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura de las particiones del conjunto de datos y preprocesamiento\n",
    "\n",
    "Contiene el código implementado para la lectura de las particiones del conjunto de datos que se va a utilizar y la obtención de sus imágenes para la experimentación y su depuración (La ejecución de este código será necesaria para la obtención de las imágenes descodificadas y su almacenamiento de una manera local para cada una de las particiones, proceso que requiere de tiempo y de espacio en memoria, 12GB)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lectura de los archivos TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista para almacenar los fragmentos de datos\n",
    "data_chunks = []\n",
    "\n",
    "# Lee los datos en fragmentos y los almacena en la lista. El nombre del archivo deberá ser cambiado dependiendo de que partición se quiera leer y obtener sus imágenes\n",
    "# Posibles nombres de archivos: snli_ve_train.tsv, snli_ve_dev.tsv, snli_ve_test.tsv\n",
    "for chunk in pd.read_csv('Datos/snli_ve_test.tsv', chunksize=1000, sep='\\t', header=None):\n",
    "    data_chunks.append(chunk)\n",
    "\n",
    "# Combina todos los fragmentos en un solo DataFrame si es necesario\n",
    "tsv_data_test = pd.concat(data_chunks)\n",
    "\n",
    "# Definir las columnas para cada uno de los archivos\n",
    "\n",
    "tsv_data_test.columns = ['Index', 'ID', 'Image_Base64', 'Hypothesis', 'Premise', 'Label']\n",
    "\n",
    "\"\"\"\n",
    "images = []\n",
    "for i in range(len(tsv_data_train)):\n",
    "    base64_str = tsv_data_train['Image_Base64'][i]\n",
    "    img_data = base64.b64decode(base64_str)\n",
    "    img = Image.open(BytesIO(img_data))\n",
    "    images.append(img)\n",
    "\n",
    "tsv_data_train['Image'] = images\n",
    "\n",
    "del images\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tsv_data_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Código para guardar las imágenes en local (primera vez que se lee de los TSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveTSVImages(tsv_data, dir):\n",
    "    # guardar las imágenes en un directorio\n",
    "    img_dir = dir\n",
    "    os.makedirs(img_dir, exist_ok=True)\n",
    "    images = tsv_data['Image']\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        img.save(os.path.join(img_dir, f'image_{i}.jpg'))\n",
    "\n",
    "# Guardar las imágenes en un directorio local de nombre \"dir\" (las imágenes ocupan mucho espacio y es un proceso que tarda tiempo). Al final se crea un archivo csv con las metáforas \n",
    "# y la ruta de la imagen al directorio local, para su posterior uso en la experimentación.\n",
    "def createNewDF(tsv_data, dir, name):\n",
    "\n",
    "    saveTSVImages(tsv_data, dir)\n",
    "    # Guardar las premisas\n",
    "    metaphors = tsv_data['Premise']\n",
    "\n",
    "    # Liberar la memoria\n",
    "    del tsv_data\n",
    "    gc.collect()\n",
    "\n",
    "    new_data = pd.DataFrame({'Metaphor': metaphors, 'Image': [os.path.join(dir, f'image_{i}.jpg') for i in range(len(metaphors))]})\n",
    "    new_data.to_csv(f'Datos/{name}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Para la obtención de las tres particiones de datos (train, dev, test) se debe ejecutar el siguiente código, cambiando el nombre del archivo y el directorio de guardado de las imágenes\n",
    "createNewDF(tsv_data_test, 'Datos/ImagesTest', 'testData')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINE-TUNING\n",
    "\n",
    "Contiene las funciones necesarias para la codificación de imágenes, de texto, cálculo de similitudes y una clase Dataset personalizada para este contexto. Adicionalmente, se encuentra el código necesario para el entrenamiento sobre HAIVMet y la obtención del mejor modelo. Por último, también se encuentra implementado el código necesario para la evaluación de recuperación de imágenes basada en texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git --quiet\n",
    "import clip\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class image_metaphor_dataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.images = df[\"Image\"].tolist()\n",
    "        self.metaphors = df[\"Metaphor\"].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metaphors)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        image = preprocess(Image.open(self.images[idx]))\n",
    "        metaphor = self.metaphors[idx]\n",
    "\n",
    "        return image, metaphor\n",
    "\n",
    "# Codifica todas las imágenes del dataloader y devuelve un tensor\n",
    "def encode_images(dataloader, model):\n",
    "    image_features = None\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            images, _ = batch\n",
    "            images = torch.squeeze(images).to(device)\n",
    "            encoded_images = model.encode_image(images).float()\n",
    "            if image_features is None:\n",
    "                image_features = encoded_images\n",
    "            else:\n",
    "                image_features = torch.cat((image_features, encoded_images))\n",
    "            #print(\"batch:\", i, \"de\", len(dataloader) - 1, \"batches procesados.\")\n",
    "\n",
    "    return image_features\n",
    "\n",
    "# Normaliza los embeddings y calcula las similitudes coseno\n",
    "def compute_similarities(text_features, image_features):\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarities = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
    "\n",
    "    return similarities\n",
    "\n",
    "\n",
    "# Evaluación de recuperación de imágenes basa en texto\n",
    "def evaluate(model, dataloader, df):\n",
    "    predicted = False\n",
    "    correct_predicted = []\n",
    "    incorrect_predicted = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    image_features = encode_images(dataloader, model)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            _, metaphors = batch\n",
    "            tokenized_metaphors = clip.tokenize(metaphors).to(device)\n",
    "            text_features = model.encode_text(tokenized_metaphors).float()\n",
    "            similarities = compute_similarities(text_features, image_features)\n",
    "            for i, metaphor in enumerate(metaphors):\n",
    "                # Obtener el índice de la imagen con mayor similitud\n",
    "                index = np.argmax(similarities[i])\n",
    "                met, _ = df.iloc[index]\n",
    "\n",
    "                # Si la metáfora que consigo con el índice de la imagen de mayour similitud es igual a la metáfora que estoy procesando, entonces acierto\n",
    "                if met == metaphor:\n",
    "                    correct += 1\n",
    "                    predicted = True\n",
    "                    if metaphor not in correct_predicted:\n",
    "                        correct_predicted.append(metaphor)\n",
    "                total += 1\n",
    "                if not predicted and metaphor not in incorrect_predicted:\n",
    "                    incorrect_predicted.append(metaphor)\n",
    "                predicted = False\n",
    "\n",
    "\n",
    "    #print('total de metáforas procesadas:', total)\n",
    "\n",
    "    return correct / total, correct_predicted, incorrect_predicted\n",
    "\n",
    "\n",
    "#https://github.com/openai/CLIP/issues/57\n",
    "def convert_models_to_fp32(model):\n",
    "    for p in model.parameters():\n",
    "        p.data = p.data.float()\n",
    "        p.grad.data = p.grad.data.float()\n",
    "\n",
    "# Función para evaluar el loss en el conjunto de validación\n",
    "def evaluate_loss(model, dataloader, loss_img, loss_txt):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            images, texts = batch\n",
    "            images = images.to(device)\n",
    "            texts = clip.tokenize(texts).to(device)\n",
    "\n",
    "            logits_per_image, logits_per_text = model(images, texts)\n",
    "\n",
    "            ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
    "\n",
    "            loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth)) / 2\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bucle de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, df_train, df_dev):\n",
    "    \n",
    "    # establecer seed\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "\n",
    "    training_dataset = image_metaphor_dataset(df_train)\n",
    "    training_dataloader = DataLoader(training_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    dev_dataset = image_metaphor_dataset(df_dev)\n",
    "    dev_dataloader = DataLoader(dev_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    if device == \"cpu\":\n",
    "      model.float()\n",
    "    else :\n",
    "      clip.model.convert_weights(model) # Actually this line is unnecessary since clip by default already on float16\n",
    "\n",
    "    loss_img = nn.CrossEntropyLoss()\n",
    "    loss_txt = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-6, betas=(0.9,0.98), eps=1e-6, weight_decay=0.4)\n",
    "\n",
    "    #scheduler = lr_scheduler.CosineAnnealingLR(optimizer, epochs, eta_min=0)\n",
    "\n",
    "    train_losses = []\n",
    "    dev_losses = []\n",
    "    \n",
    "    losses_epoch = 0.0\n",
    "\n",
    "    best_dev_loss = float('inf')\n",
    "    best_dev_epoch = 0\n",
    "    start_time=time.time()\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        losses_epoch = 0.0\n",
    "        pbar = tqdm(training_dataloader, total=len(training_dataloader))\n",
    "        for batch in pbar:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            images,texts = batch\n",
    "\n",
    "            images= images.to(device)\n",
    "            texts = clip.tokenize(texts).to(device)\n",
    "\n",
    "            logits_per_image, logits_per_text = model(images, texts)\n",
    "\n",
    "            ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "\n",
    "            total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "            losses_epoch += total_loss.item()\n",
    "            total_loss.backward()\n",
    "\n",
    "            if device == \"cpu\":\n",
    "              optimizer.step()\n",
    "            else :\n",
    "              convert_models_to_fp32(model)\n",
    "              optimizer.step()\n",
    "              clip.model.convert_weights(model)\n",
    "\n",
    "            pbar.set_description(f\"Epoch {epoch}/{epochs-1}\")\n",
    "\n",
    "        #scheduler.step()\n",
    "        \n",
    "        # print el loss promedio de cada epoch\n",
    "        print(\"el loss promedio de la epoch\", epoch, \"es:\", losses_epoch/len(training_dataloader))\n",
    "        train_losses.append(losses_epoch/len(training_dataloader))\n",
    "\n",
    "        dev_loss = evaluate_loss(model, dev_dataloader, loss_img, loss_txt)\n",
    "        print(\"el loss de dev en la epoch\", epoch, \"es:\", dev_loss)\n",
    "        print(\"\\n\")\n",
    "        dev_losses.append(dev_loss)\n",
    "        \n",
    "        if dev_loss < best_dev_loss:\n",
    "            best_dev_loss = dev_loss\n",
    "            best_dev_epoch = epoch\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "    print(\"\\nBest Performing Model achieves dev loss of : {:.4f} in epoch: {:.1f}\".format(best_dev_loss, best_dev_epoch))\n",
    "    print(\"Time: {:.3f} seconds\".format(time.time() - start_time))\n",
    "\n",
    "  \n",
    "    # Plot los valores de loss de train y dev por épocas\n",
    "    plt.plot(train_losses, label='train')\n",
    "    plt.plot(dev_losses, label='dev')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('Datos/testData.csv')\n",
    "dev_data = pd.read_csv('Datos/devData.csv')\n",
    "train_data = pd.read_csv('Datos/trainData.csv')\n",
    "\n",
    "print(\"test data:\", len(test_data))\n",
    "print(\"dev data:\", len(dev_data))\n",
    "print(\"train data:\", len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load('ViT-B/32', device=device, jit=False) #Must set jit=False for training\n",
    "\n",
    "# Train\n",
    "train(model=model, epochs=20, df_train=train_data, df_dev=dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluación del mejor modelo obtenido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "test_dataset = image_metaphor_dataset(test_data)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "train_dataset = image_metaphor_dataset(train_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "dev_dataset = image_metaphor_dataset(dev_data)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "train_acc = evaluate(model, train_dataloader, train_data)\n",
    "print('Train accuracy: {:.2f}'.format(train_acc))\n",
    "\n",
    "dev_acc = evaluate(model, dev_dataloader, dev_data)\n",
    "print('Dev accuracy: {:.2f}'.format(dev_acc))\n",
    "\n",
    "test_acc = evaluate(model, test_dataloader, test_data)\n",
    "print('Test accuracy: {:.2f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paráfrasis\n",
    "\n",
    "Generación de las paráfrasis literales sobre las metáforas del conjunto de test, comprende la lectura, generación de paráfrasis y creación de un TSV con los pares metáforas-paráfrasis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de las metáforas textuales del conjunto de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('Datos/testData.csv')\n",
    "print(\"test data:\", len(test_data))\n",
    "test_data.head(171)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_test_metaphors = test_data['Metaphor'].unique()\n",
    "print(\"Número de metáforas únicas en el conjunto de test:\", len(unique_test_metaphors))\n",
    "#print(unique_test_metaphors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohere\n",
    "\n",
    "Proceso de generación de las paráfrasis literal para cada una de las metáforas textuales del conjunto de test utilizando Command R-Plus de Cohere(aquí no ejecutar nada, se utilizará el tsv ya creado con las metáforas-paráfrasis en su lugar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para los values de las metáforas nos conectamos a la API de Coral para obtener las paráfrasis de las metáforas únicas\n",
    "%pip install cohere --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Código para conectarse con la API de Cohere y obtener las paráfrasis de las metáforas de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "co = cohere.Client('placeholder') # Pon tu API key de Coral\n",
    "\n",
    "test_paraphrases = {}\n",
    "chat_history = []\n",
    "\n",
    "for met in unique_test_metaphors:\n",
    "    #met = 'We were sinking in an ocean of grass'\n",
    "    # Mensaje al bot de Coral\n",
    "    message = 'Please provide a short literal paraphrase of the following metaphor: ' + met + \". The answer that you provide me with must only contain the literal paraphrase that you generate, nothing else.\"\n",
    "\n",
    "    # Generar una respuesta con el historial de chat\n",
    "\n",
    "    response = co.chat(\n",
    "        message = message,\n",
    "        model=\"command\",\n",
    "        temperature=0.1,\n",
    "    )\n",
    "\n",
    "    # Obtener la respuesta del bot\n",
    "    answer = response.text\n",
    "\n",
    "    test_paraphrases[met] = answer\n",
    "\n",
    "    time.sleep(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genero un TSV con las columnas de Metaphor y Paraphrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generar un archivo CSV con las metáforas únicas y sus paráfrasis\n",
    "test_paraphrases_df = pd.DataFrame(test_paraphrases.items(), columns=['Metaphor', 'Paraphrase'])\n",
    "# Guardar como TSV\n",
    "test_paraphrases_df.to_csv('Datos/testParaphrases.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura TSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lectura del TSV de pares metáforas-paráfrasis. Sobre estos también se encuentra realiza el análisis manual de objetos relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_paraphrases_df = pd.read_csv('Datos/testParaphrases.tsv', sep='\\t')\n",
    "print(len(test_paraphrases_df))\n",
    "test_paraphrases_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objetos_niguno = test_paraphrases_df[test_paraphrases_df['Ninguno'] == 'X']\n",
    "objetos_al_menos_1 = test_paraphrases_df[test_paraphrases_df['al menos 1'] == 'X']\n",
    "objetos_todos = test_paraphrases_df[test_paraphrases_df['Todos'] == 'X']\n",
    "\n",
    "print(\"Número de paráfrasis con ningún objeto:\", len(objetos_niguno))\n",
    "print(\"Número de paráfrasis con al menos 1 objeto:\", len(objetos_al_menos_1))\n",
    "print(\"Número de paráfrasis con todos los objetos:\", len(objetos_todos))\n",
    "\n",
    "print(\"Porcentaje de paráfrasis con ningún objeto:\", round((len(objetos_niguno) / len(test_paraphrases_df)) * 100, 2))\n",
    "print(\"Porcentaje de paráfrasis con al menos 1 objeto:\", round((len(objetos_al_menos_1) / len(test_paraphrases_df)) * 100, 2))\n",
    "print(\"Porcentaje de paráfrasis con todos los objetos:\", round((len(objetos_todos) / len(test_paraphrases_df)) * 100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sustituyo las metáforas por las paráfrasis en un nuevo dataframe constituido de paráfrasis-imagen para la experimentación de recuperación de imágenes basada en texto con paráfrasis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Obtener las paráfrasis\n",
    "test_paraphrases = test_paraphrases_df['Paraphrase'].tolist()\n",
    "test_metaphors = test_paraphrases_df['Metaphor'].tolist()\n",
    "\n",
    "# Leo del TSV\n",
    "test_data_paraphrased = pd.read_csv('Datos/testParaphrasesPrueba.tsv', sep='\\t')\n",
    "\n",
    "test_data_paraphrased.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentación\n",
    "\n",
    "Incluye las pruebas realizadas para recuperación de imágenes basada en texto con los pares metáfora-imagen y paráfrasis-imagen. También contiene las pruebas para VLE y sus diferentes tipos de evaluación. Toda la experimentación se realiza sobre los modelos CLIP zero-shot y fine-tuned en HAIVMet. adicionalmente, comprende la generación del subconjunto diferencia y su análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/openai/CLIP.git --quiet\n",
    "import clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de recuperación de imágenes basada en texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para la obtención del conjunto diferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diferenciaConjuntos(correct_metaphors, correct_paraphrases, test_paraphrases_df):\n",
    "    diferencia = []\n",
    "    paraphrases_to_metaphors = []\n",
    "\n",
    "    # Obtener las metáforas correspondientes de correct_paraphrases\n",
    "    for i, paraphrase in enumerate(correct_paraphrases):\n",
    "        metafora = test_paraphrases_df[test_paraphrases_df['Paraphrase'] == paraphrase].values[0][0]\n",
    "        paraphrases_to_metaphors.append(metafora)\n",
    "        #print(metafora)\n",
    "        \n",
    "\n",
    "    # obtener la diferencia de conjuntos entre las listas correct_metaphors y paraphrases_to_metaphors\n",
    "    correct_metaphors = set(correct_metaphors)\n",
    "    paraphrases_to_metaphors = set(paraphrases_to_metaphors)\n",
    "    diferencia = correct_metaphors.difference(paraphrases_to_metaphors)\n",
    "\n",
    "    return diferencia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluación zero-shot y de los conjuntos metáforasTextuales/imágenes originales y después paráfrasis/imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar un CLIP preentrenado\n",
    "model, preprocess = clip.load('ViT-B/32', device=device)\n",
    "\n",
    "test_data = pd.read_csv('Datos/testData.csv')\n",
    "\n",
    "test_dataset = image_metaphor_dataset(test_data)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# evaluate test data\n",
    "test_acc, correct_metaphors, incorrect_metaphors = evaluate(model, test_dataloader, test_data)\n",
    "print(\"Zero-shot Test accuracy pares originales metáforas/imágenes: \", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar un CLIP preentrenado\n",
    "model, preprocess = clip.load('ViT-B/32', device=device)\n",
    "\n",
    "# Crear un dataset con las metáforas de test_data_paraphrased\n",
    "test_dataset_paraphrased = image_metaphor_dataset(test_data_paraphrased)\n",
    "\n",
    "test_dataloader_paraphrased = DataLoader(test_dataset_paraphrased, batch_size=64, shuffle=False)\n",
    "\n",
    "# Evaluar el modelo con las metáforas de test_data_paraphrased\n",
    "test_acc_paraphrased, correct_paraphrases, incorrect_paraphrases = evaluate(model, test_dataloader_paraphrased, test_data_paraphrased)\n",
    "print(\"Zero-shot Test accuracy con las paráfrasis: \", test_acc_paraphrased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtención del subconjunto diferencia para el enfoque zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diferencia = diferenciaConjuntos(correct_metaphors, correct_paraphrases, test_paraphrases_df)\n",
    "print(\"Diferencia entre las metáforas correctas y las paráfrasis correctas:\", diferencia)\n",
    "print(len(diferencia))\n",
    "\n",
    "\n",
    "# Crear un TSV con las columnas correct metaphors, correct paraphrases y diferencia\n",
    "diferencia_df = pd.DataFrame(diferencia ,columns=['Diferencia de Metáforas'])\n",
    "diferencia_df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluación de los conjuntos metáforasTextuales/imágenes originales y después paráfrasis/imágenes con el modelo Fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el CLIP finetuneado\n",
    "model, _ = clip.load('ViT-B/32', device=device)\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Evaluación con el modelo finetuneado con las metáforas de test_data\n",
    "test_acc_finetuned = evaluate(model, test_dataloader, test_data)\n",
    "print(\"Test accuracy con los pares originales metáforas/imágenes después del fine-tuning: \", test_acc_finetuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación con el modelo finetuneado con las paráfrasis de test_data_paraphrased\n",
    "model, _ = clip.load('ViT-B/32', device=device)\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "test_acc_finetuned = evaluate(model, test_dataloader_paraphrased, test_data_paraphrased)\n",
    "print(\"Test accuracy con las paráfrasis después del fine-tuning: \", test_acc_finetuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VLE Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocesamiento de los datos de VLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura del archivo TSV\n",
    "tsv_data_test = pd.read_csv('Datos/snli_ve_test.tsv', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los nombres de las columnas\n",
    "tsv_data_test.columns = ['Index', 'ID', 'Image_Base64', 'Hypothesis', 'Premise', 'Label']\n",
    "\n",
    "# Decodificar las imágenes base64 y guardarlas en una lista\n",
    "images = []\n",
    "for i in range(len(tsv_data_test)):\n",
    "    base64_str = tsv_data_test['Image_Base64'][i]\n",
    "    img_data = base64.b64decode(base64_str)\n",
    "    img = Image.open(BytesIO(img_data))\n",
    "    images.append(img)\n",
    "\n",
    "# Agregar la lista de imágenes al DataFrame\n",
    "tsv_data_test['Image'] = images\n",
    "\n",
    "# Eliminar la columna 'Image_Base64'\n",
    "tsv_data_test.drop(columns=['Image_Base64'], inplace=True)\n",
    "\n",
    "print(len(tsv_data_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluación Zero-shot primero de manera Estricta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conseguir premisas únicas\n",
    "premises = tsv_data_test['Premise'].unique()\n",
    "\n",
    "# Obtain the strict VE data\n",
    "strict_VE_data = format_VE_strict(tsv_data_test, premises)\n",
    "print(\"pares texto-imagen: \", len(strict_VE_data))\n",
    "strict_VE_data_premises = strict_VE_data['Premise'].unique()\n",
    "print(\"premisas textuales únicas: \", len(strict_VE_data_premises))\n",
    "\n",
    "# obtain unique images\n",
    "images = strict_VE_data['Image']\n",
    "images = images.tolist()\n",
    "unique_imgs_strict = obtainUniqueImages(images)\n",
    "print(len(unique_imgs_strict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo CLIP\n",
    "model, preprocess = clip.load('ViT-B/32', device=device)\n",
    "\n",
    "# evaluate test data\n",
    "strict_test_acc = evaluateStrictVE(unique_imgs_strict, strict_VE_data)\n",
    "print(f\"Zero-shot Test accuracy VLE Estricto: {strict_test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluación Zero-shot de manera Flexible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the flexible VE data\n",
    "flexible_VE_data = format_VE_flexible(tsv_data_test, premises)\n",
    "print(\"pares texto-imagen: \", len(flexible_VE_data))\n",
    "flexible_VE_data_premises = flexible_VE_data['Premise'].unique()\n",
    "print(\"premisas textuales únicas: \", len(flexible_VE_data_premises))\n",
    "\n",
    "# Obtain unique images\n",
    "images = flexible_VE_data['Image']\n",
    "images = images.tolist()\n",
    "unique_imgs_flexible = obtainUniqueImages(images)\n",
    "print(len(unique_imgs_flexible))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo CLIP\n",
    "model, preprocess = clip.load('ViT-B/32', device=device)\n",
    "\n",
    "flexible_test_acc = evaluateFlexibleVE(unique_imgs_flexible, flexible_VE_data)\n",
    "print(f\"Zero-shot Test accuracy VLE Flexible: {flexible_test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluación con el modelo Fine-tuned, primero de manera estricta y luego flexible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo fine-tuneado\n",
    "model, preprocess = clip.load('ViT-B/32', device=device)\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "\n",
    "# Evaluación con el modelo finetuneado Estricta\n",
    "strict_test_acc_finetuned = evaluateStrictVE(unique_imgs_strict, strict_VE_data)\n",
    "print(f\"Test accuracy VLE Estricto después del fine-tuning: {strict_test_acc_finetuned:.2f}\")\n",
    "\n",
    "\n",
    "# Evaluación con el modelo finetuneado Flexible\n",
    "flexible_test_acc_finetuned = evaluateFlexibleVE(unique_imgs_flexible, flexible_VE_data)\n",
    "print(f\"Test accuracy VLE Flexible después del fine-tuning: {flexible_test_acc_finetuned:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
